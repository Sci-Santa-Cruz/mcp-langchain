{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7b187c-872e-4438-b680-f08bedc17df5",
   "metadata": {},
   "source": [
    "## Interacción con las capacidades del servidor MCP\n",
    "\n",
    "Los **servidores MCP** pueden proporcionar una amplia variedad de capacidades, incluyendo las primitivas **tools**, **resources** y **prompts**.  \n",
    "Como desarrollador de clientes, se interactúa con ellas a través del objeto **ClientSession** instanciado, y por lo general se utiliza el mismo flujo de trabajo de **dos pasos** para cada una de ellas: **descubrimiento** y **uso**.\n",
    "\n",
    "Para cada una de estas capacidades, el descubrimiento se realiza mediante una solicitud **`<primitive>/list`**, que en el **SDK de Python** está encapsulada en los métodos **`list_<primitive>s()`**.  \n",
    "Estas llamadas —si el servidor proporciona la capacidad correspondiente— devolverán una lista de las **herramientas**, **recursos** o **prompts** disponibles.\n",
    "\n",
    "El uso de las capacidades resultantes requiere una llamada **`<primitive>/<verb>`**, donde el *verbo* depende del tipo de primitivo que se llama.  \n",
    "En el SDK de Python, estas llamadas están encapsuladas con métodos **`<verb>_<primitive>()`**, como **`call_tool()`** para **`tool/call`**.\n",
    "\n",
    "---\n",
    "\n",
    "### Nota\n",
    "\n",
    "El protocolo también permite que el **servidor** (y el **cliente**) envíen mensajes de **notificación**.  \n",
    "Posteriormente, el receptor gestiona las notificaciones según su conveniencia.  \n",
    "En las conexiones **HTTP Streamable**, estas notificaciones se enviarán como cualquier otro mensaje, según lo definido por el transporte, pero como una implementación **`JSONRPCNotification`** de la clase **`JSONRPCMessage`**.\n",
    "\n",
    "---\n",
    "\n",
    "### Buenas prácticas de desarrollo\n",
    "\n",
    "Si bien los SDK MCP disponibles envuelven las distintas llamadas de solicitud en métodos que el cliente puede invocar directamente, a menudo es recomendable **envolver estos métodos en funciones propias**.  \n",
    "Esto permite agregar:\n",
    "\n",
    "- **Registros (logging)**  \n",
    "- **Reintentos automáticos**  \n",
    "- **Parámetros adicionales de método**  \n",
    "- **Otras personalizaciones** a las llamadas al servidor  \n",
    "\n",
    "Por ejemplo, se puede pasar cualquiera de los métodos **`list_<primitive>()`** al parámetro opcional **`cursor`** para manejar **paginación**.\n",
    "\n",
    "---\n",
    "\n",
    "## Herramientas\n",
    "\n",
    "Las **herramientas (tools)** son el recurso más común que ofrecen los servidores MCP —y con razón—, ya que los **flujos de trabajo agénticos** suelen requerir herramientas para **ampliar las capacidades de un LLM básico**.\n",
    "\n",
    "Una **herramienta** es simplemente una **función determinista** que un LLM puede decidir llamar si lo considera necesario.  \n",
    "Naturalmente, esta función puede hacer **cualquier cosa que pueda codificarse**, desde calcular números hasta consultar el clima en una ubicación determinada.  \n",
    "Los servidores MCP permiten a los clientes **descubrir y utilizar estas herramientas**.\n",
    "\n",
    "---\n",
    "\n",
    "### Descubrir herramientas con el SDK de Python\n",
    "\n",
    "Para descubrir herramientas, el cliente simplemente llama a:\n",
    "\n",
    "```python\n",
    "self.session.list_tools()\n",
    "````\n",
    "\n",
    "Esta llamada devuelve un objeto **`response`** con una propiedad **`tools`**,\n",
    "que contiene la lista de herramientas disponibles en el servidor conectado al cliente.\n",
    "\n",
    "Cada herramienta incluye las siguientes propiedades:\n",
    "\n",
    "* `name`\n",
    "* `description`\n",
    "* `inputSchema`\n",
    "* `annotations`\n",
    "* `model_config`\n",
    "\n",
    "Puedes consultar la estructura actual de la clase **`Tool`** en el módulo de tipos del SDK de Python de MCP.\n",
    "\n",
    "---\n",
    "\n",
    "### Extender el uso de `list_tools()`\n",
    "\n",
    "Llamar a **`list_tools()`** directamente puede ser suficiente para casos básicos,\n",
    "pero si estás construyendo un cliente que se usará en **diferentes servidores**,\n",
    "con distintas capacidades o en **diferentes aplicaciones**,\n",
    "puede ser ventajoso **escribir una función propia** que incluya:\n",
    "\n",
    "* Filtros o restricciones específicas\n",
    "* Registro (logging) detallado\n",
    "* Manejo de errores\n",
    "* Cualquier otra funcionalidad que tus usuarios puedan necesitar\n",
    "\n",
    "Por ejemplo, podrías implementar una función similar a la siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1953615-ffb0-4322-8c46-f34e767c9300",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### `client.py`\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from typing import Any\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MCPClient:\n",
    "    ...\n",
    "    async def get_available_tools(self) -> list[dict[str, Any]]:\n",
    "        if not self._connected:\n",
    "            raise RuntimeError(\"Client not connected to a server\")\n",
    "\n",
    "        tools_result = await self._session.list_tools()\n",
    "\n",
    "        if not tools_result.tools:\n",
    "            logger.warning(\"No tools found on server\")\n",
    "\n",
    "        available_tools = [\n",
    "            {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"input_schema\": tool.inputSchema,\n",
    "            }\n",
    "            for tool in tools_result.tools\n",
    "        ]\n",
    "\n",
    "        return available_tools\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "### `agent.py`\n",
    "\n",
    "```python\n",
    "async def main():\n",
    "    await mcp_client.connect()\n",
    "\n",
    "    available_tools = await mcp_client.get_available_tools()\n",
    "    print(f\"Available tools: {', '.join([tool['name'] for tool in available_tools])}\")\n",
    "\n",
    "    while True:\n",
    "        ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb6aa69-88e2-4d62-a307-497d377cf56d",
   "metadata": {},
   "source": [
    "## Uso de herramientas en un cliente MCP\n",
    "\n",
    "El método anterior es un **envoltorio sencillo** alrededor del método `list_tools()` del objeto `Session`, el cual a su vez envuelve la solicitud `tools/list`.  \n",
    "Aunque es simple, el ejemplo muestra cómo implementar **pequeñas medidas de seguridad**, como verificar el estado de conexión del servidor, así como **registros más detallados**, como la comprobación de resultados vacíos de herramientas.\n",
    "\n",
    "El método también convierte la lista de objetos de herramientas nativos de MCP en una **lista de diccionarios** con las claves `\"name\"`, `\"description\"` y `\"input_schema\"`.  \n",
    "Esto garantiza **compatibilidad con la API de mensajes de Anthropic**.\n",
    "\n",
    "Puedes extender este patrón para:\n",
    "\n",
    "- Agregar **reintentos automáticos**  \n",
    "- Traducir el formato de herramientas de MCP a otro formato compatible con otro LLM  \n",
    "- O encapsular los objetos `Tool` en tus propias clases personalizadas  \n",
    "\n",
    "En el ejemplo de la función `main()`, se agregó una **list comprehension** para construir una lista con los nombres de las herramientas disponibles y se imprimió para verificar el funcionamiento del código.\n",
    "\n",
    "---\n",
    "\n",
    "## Ejecutar herramientas: `use_tool()`\n",
    "\n",
    "Una vez que tu cliente tiene herramientas disponibles, el siguiente paso natural es **usarlas**.  \n",
    "Al igual que en el ejemplo de `get_available_tools()`, ahora se envolverá el método `call_tool()` del objeto `Session`, añadiendo **registro (logging)** y **manejo de los diferentes tipos de respuesta** posibles.\n",
    "\n",
    "El método `call_tool()` devuelve un objeto **`CallToolResult`**, que contiene una lista de cualquier tipo de contenido definido en el SDK:\n",
    "\n",
    "- **`TextContent`** — El resultado de la herramienta es texto, almacenado en la propiedad `text`.  \n",
    "- **`ImageContent`** — El resultado es una imagen, almacenada como cadena **base64** en la propiedad `data`.  \n",
    "- **`AudioContent`** — El resultado es audio, también codificado en **base64** en la propiedad `data`.  \n",
    "- **`EmbeddedResource`** — La herramienta devuelve recursos embebidos, usualmente como contexto adicional o para almacenamiento en caché.  \n",
    "  - Puede contener un `TextResourceContents` (texto de un archivo o configuración).  \n",
    "  - O un `BlobResourceContents` (datos binarios en formato **base64**).\n",
    "\n",
    "> **Importante:** las solicitudes de uso de herramientas devuelven **listas de contenidos**, y en algunos casos —por ejemplo, cuando se usa un `EmbeddedResource` como contexto adicional— esto debe esperarse y manejarse correctamente.\n",
    "\n",
    "A continuación se muestra un ejemplo completo de cómo envolver `call_tool()` y devolver **representaciones en cadena** de cada tipo de contenido válido.\n",
    "\n",
    "---\n",
    "\n",
    "### `client.py`\n",
    "\n",
    "```python\n",
    "from typing import Any\n",
    "from mcp.types import TextResourceContents\n",
    "\n",
    "class MCPClient:\n",
    "    ...\n",
    "    async def use_tool(self, tool_name: str, arguments: dict[str, Any] | None = None) -> list[str]:\n",
    "        if not self._connected:\n",
    "            raise RuntimeError(\"Client not connected to a server\")\n",
    "\n",
    "        tool_call_result = await self._session.call_tool(name=tool_name, arguments=arguments)\n",
    "        logger.debug(f\"Calling tool {tool_name} with arguments {arguments}\")\n",
    "\n",
    "        results = []\n",
    "        if tool_call_result.content:\n",
    "            for content in tool_call_result.content:\n",
    "                match content.type:\n",
    "                    case \"text\":\n",
    "                        results.append(content.text)\n",
    "                    case \"image\" | \"audio\":\n",
    "                        results.append(content.data)\n",
    "                    case \"resource\":\n",
    "                        if isinstance(content.resource, TextResourceContents):\n",
    "                            results.append(content.resource.text)\n",
    "                        else:\n",
    "                            results.append(content.resource.blob)\n",
    "        else:\n",
    "            logger.warning(f\"No content in tool call result for tool {tool_name}\")\n",
    "\n",
    "        return results\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "### Explicación del método\n",
    "\n",
    "El método **`use_tool()`**, al igual que `list_tools()`, primero:\n",
    "\n",
    "1. **Verifica la conexión** del cliente al servidor MCP.\n",
    "2. Llama a **`call_tool()`**, pasando el nombre de la herramienta y sus argumentos.\n",
    "3. Registra la acción con un mensaje de depuración (`logger.debug`).\n",
    "\n",
    "Luego:\n",
    "\n",
    "* Si la respuesta contiene contenido (`content`), itera sobre cada elemento y **evalúa su tipo** (`text`, `image`, `audio`, `resource`).\n",
    "* Convierte cada tipo en una **representación de texto o base64** y la añade a la lista de resultados.\n",
    "* Si la respuesta está vacía, genera una advertencia (`logger.warning`).\n",
    "* Finalmente, **devuelve la lista de resultados**.\n",
    "\n",
    "Este patrón permite **control total sobre el flujo de ejecución** y el manejo de resultados en las llamadas a herramientas.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ Advertencias importantes\n",
    "\n",
    "* En algunos casos, **manejar los tipos de respuesta en el cliente** puede no ser lo ideal.\n",
    "  Antes de hacerlo, considera si tus usuarios necesitan controlar cómo se gestionan los diferentes tipos de contenido.\n",
    "\n",
    "* Cuando construyas **agentes que soporten MCP**, monitorea cuidadosamente:\n",
    "\n",
    "  * El **rendimiento general** del agente.\n",
    "  * La **eficiencia del llamado a herramientas**.\n",
    "\n",
    "  A medida que se añaden más herramientas, el rendimiento tiende a disminuir debido a:\n",
    "\n",
    "  * **Ambigüedad u overlap** entre descripciones de herramientas.\n",
    "  * **Interfaces demasiado detalladas**, que aumentan la carga cognitiva del LLM para seleccionar la herramienta correcta.\n",
    "\n",
    "---\n",
    "\n",
    "## Integración con modelos de lenguaje\n",
    "\n",
    "Si estás construyendo un cliente para una aplicación específica (por ejemplo, un **chat con IA**), también deberás **utilizar estas llamadas**.\n",
    "\n",
    "Muchas APIs de LLM que soportan el llamado a herramientas tienen un parámetro opcional **`tools`**, que permite enviar fácilmente las herramientas disponibles al modelo.\n",
    "Otros modelos requieren **incrustar la lista de nombres, descripciones y esquemas** en el *system prompt*.\n",
    "\n",
    "En este ejemplo nos centramos en la **API de Claude de Anthropic**, la cual **sí acepta un parámetro `tools`**.\n",
    "Este parámetro se añade a los mensajes del usuario enviados al modelo; el host analiza las respuestas buscando invocaciones de herramientas y, si las encuentra, construye un **mensaje `tool_result`** que se agrega al historial (`messages`) antes de generar el resultado final para el usuario.\n",
    "\n",
    "Así, el modelo puede **acceder a las herramientas proporcionadas por el servidor MCP** y **llamarlas dinámicamente** cuando lo requiera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f8a90-10ae-4ce7-8a0f-bd0dc385d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "\n",
    "mcp_client = MCPClient(\n",
    "    name=\"calculator_server_connection\",\n",
    "    command=\"uv\",\n",
    "    server_args=[\n",
    "        \"--directory\",\n",
    "        str(Path(__file__).parent.resolve()),\n",
    "        \"run\",\n",
    "        \"calculator_server.py\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Bienvenido a tu Asistente de IA. Escribe 'adiós' para salir.\")\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Función principal asincrónica que ejecuta el asistente.\"\"\"\n",
    "    try:\n",
    "        await mcp_client.connect()\n",
    "        available_tools = await mcp_client.get_available_tools()\n",
    "        print(\n",
    "            f\"Herramientas disponibles: {', '.join([tool['name'] for tool in available_tools])}\"\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            prompt = input(\"Tú: \")\n",
    "            if prompt.lower() == \"adiós\":\n",
    "                print(\"Asistente IA: ¡Hasta luego!\")\n",
    "                break\n",
    "\n",
    "            # Construir la conversación iniciando con el mensaje del usuario\n",
    "            conversation_messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "            # Bucle de uso de herramientas: continúa hasta obtener una respuesta final de texto\n",
    "            while True:\n",
    "                # Obtener respuesta del modelo LLM\n",
    "                current_response = anthropic_client.messages.create(\n",
    "                    max_tokens=4096,\n",
    "                    messages=conversation_messages,\n",
    "                    model=\"claude-sonnet-4-0\",\n",
    "                    tools=available_tools,\n",
    "                    tool_choice={\"type\": \"auto\"},\n",
    "                )\n",
    "\n",
    "                # Agregar mensaje del asistente a la conversación\n",
    "                conversation_messages.append(\n",
    "                    {\"role\": \"assistant\", \"content\": current_response.content}\n",
    "                )\n",
    "\n",
    "                # Verificar si es necesario usar herramientas\n",
    "                if current_response.stop_reason == \"tool_use\":\n",
    "                    # Extraer bloques de uso de herramientas\n",
    "                    tool_use_blocks = [\n",
    "                        block\n",
    "                        for block in current_response.content\n",
    "                        if block.type == \"tool_use\"\n",
    "                    ]\n",
    "\n",
    "                    # Ejecutar todas las herramientas y recopilar resultados\n",
    "                    tool_results = []\n",
    "                    for tool_use in tool_use_blocks:\n",
    "                        print(f\"Usando herramienta: {tool_use.name}\")\n",
    "                        tool_result = await mcp_client.use_tool(\n",
    "                            tool_name=tool_use.name, arguments=tool_use.input\n",
    "                        )\n",
    "                        tool_results.append(\n",
    "                            {\n",
    "                                \"type\": \"tool_result\",\n",
    "                                \"tool_use_id\": tool_use.id,\n",
    "                                \"content\": \"\\n\".join(tool_result),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    # Agregar resultados de herramientas a la conversación\n",
    "                    conversation_messages.append(\n",
    "                        {\"role\": \"user\", \"content\": tool_results}\n",
    "                    )\n",
    "\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    # No se requieren herramientas, extraer la respuesta final en texto\n",
    "                    text_blocks = [\n",
    "                        content.text\n",
    "                        for content in current_response.content\n",
    "                        if hasattr(content, \"text\") and content.text.strip()\n",
    "                    ]\n",
    "\n",
    "                    if text_blocks:\n",
    "                        print(f\"Asistente: {text_blocks[0]}\")\n",
    "                    else:\n",
    "                        print(\"Asistente: [No hay respuesta de texto disponible]\")\n",
    "\n",
    "                    break\n",
    "    finally:\n",
    "        await mcp_client.disconnect()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9eb310-6360-41de-8930-0abd829bbe7e",
   "metadata": {},
   "source": [
    "Este código tiene varios cambios para manejar la obtención y el uso de las herramientas necesarias. Primero, durante la fase de inicialización, obtenemos todas las herramientas disponibles del servidor al que estamos conectados mediante `get_available_tools()`. Luego, transformamos la lista de objetos `Tool` en una lista de diccionarios simples para poder devolverla correctamente al modelo. Esto se hace al inicio del módulo para obtener la lista de herramientas solo una vez. Si esperas que el servidor o los servidores a los que te conectas cambien las herramientas que ofrecen mientras estás conectado, deberás obtener una nueva lista de herramientas disponibles en cada entrada del usuario al modelo o hacer que tu cliente escuche y maneje una notificación `list_changed` del servidor.\n",
    "\n",
    "A continuación, actualizamos el código donde creamos el mensaje del usuario para el modelo, pasando la lista de herramientas al parámetro `tools`, y configurando `tool_choice` como un diccionario con la clave `\"type\"` establecida en `\"auto\"`. No es obligatorio hacerlo, ya que `\"auto\"` es la configuración predeterminada y permite que el modelo decida qué herramienta o herramientas usar para responder la consulta del usuario. También puedes establecerlo en `\"any\"`, donde usará cualquiera (pero al menos una) de las herramientas disponibles, o en un nombre de herramienta y tipo `\"tool\"` para forzar al modelo a usar una herramienta específica, o `\"none\"` para evitar que el modelo use cualquier herramienta disponible.\n",
    "\n",
    "Después de esto, inspeccionamos el resultado e inicializamos un posible mensaje de uso de herramienta que será enviado de vuelta al modelo. Si la respuesta del mensaje tiene un `stop_reason` de `tool_use`, entonces sabemos que el modelo está esperando una respuesta con los resultados de la o las herramientas que está solicitando. Creamos bloques de mensajes de uso de herramienta a partir del contenido de la respuesta y luego ejecutamos todas las herramientas en un bucle. A partir de los resultados, construimos un diccionario con la clave `type` establecida en `\"tool_use\"`, `tool_use_id` establecida en el ID de la respuesta de uso de herramienta del modelo, y `content` con el resultado de la llamada a la herramienta. Todos estos se agregan luego a la lista `conversation_messages`. Una vez realizadas todas las llamadas a herramientas, los resultados y la pregunta original del usuario se envían de vuelta al LLM para su evaluación. Dado que potencialmente pueden realizarse múltiples llamadas a herramientas antes de devolver una respuesta al usuario (como preguntar al modelo cuánto es 5 * 3 + 7), envolvemos todo esto en un bucle `while`.\n",
    "\n",
    "Si no tenemos ninguna respuesta de llamada a herramienta en el mensaje de uso de herramienta, significa que no se necesitaron herramientas o que hemos llegado al final de la fase de uso de herramientas. En cualquier caso, extraemos los bloques de texto de la última respuesta y los imprimimos si están disponibles. Finalmente, todo el proceso está envuelto en un bloque `try/finally` para asegurar que la conexión se desconecte correctamente cuando el usuario cierre la aplicación.\n",
    "\n",
    "En esta sección, aprendiste sobre las herramientas proporcionadas por el servidor MCP, cómo descubrirlas en un servidor, cómo usarlas y cómo utilizarlas en el contexto de una aplicación de chat. En la siguiente sección, aprenderás sobre los recursos y cómo puedes usarlos en tu aplicación de manera similar a como lo hiciste con las herramientas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734398bf-12cf-44b3-af4f-af2887dba774",
   "metadata": {},
   "source": [
    "# Recursos\n",
    "\n",
    "Como aprendiste en el **Capítulo 2**, los **recursos MCP** pueden desempeñar muchos roles, algunos de los cuales aún no han sido completamente explorados por la comunidad, como servir de **caché para optimizar el tamaño de los *prompts***.\n",
    "\n",
    "Básicamente, los recursos permiten que una **aplicación anfitriona** y el **LLM** con el que interactúa accedan a datos proporcionados por el servidor MCP.\n",
    "Estos datos pueden ser registros de bases de datos, imágenes, archivos de texto, y más.\n",
    "\n",
    "Los recursos admiten varias **acciones**:\n",
    "\n",
    "---\n",
    "\n",
    "### Acciones principales\n",
    "\n",
    "* **resources/list** o **list_resources()**\n",
    "  Solicita al servidor una lista de los recursos disponibles.\n",
    "\n",
    "* **resources/templates/list** o **list_resource_templates()**\n",
    "  Solicita una lista de **plantillas de URI de recursos** desde el servidor.\n",
    "  Estas permiten construir dinámicamente la URI de un recurso en función de otra información que tengas.\n",
    "  En el SDK de Python, estas cadenas se asemejan a **f-strings**, con secciones variables entre llaves `{}`.\n",
    "\n",
    "* **resources/read** o **read_resource()**\n",
    "  Accede al archivo proporcionado por el servidor, enviando los datos al LLM en formato **texto** o **blob**.\n",
    "\n",
    "* **resources/subscribe** o **subscribe_resource()**\n",
    "  Suscribe al cliente a las actualizaciones del recurso cuya URI se especifica en la llamada.\n",
    "\n",
    "* **resources/unsubscribe** o **unsubscribe_resource()**\n",
    "  Cancela la suscripción del cliente a las actualizaciones del recurso especificado mediante su URI.\n",
    "\n",
    "---\n",
    "\n",
    "> **Nota**\n",
    "> Consulta el proyecto **tupac** de *Tim Kellogg* para ver una implementación minimalista de cliente que utiliza **recursos MCP como caché** para *prompting* eficiente.\n",
    "\n",
    "- https://github.com/tkellogg/tupac/\n",
    "---\n",
    "\n",
    "Primero implementaremos **wrappers** para los dos métodos de listado.\n",
    "Seguiremos el patrón establecido en la sección anterior con `get_available_tools()`: un **envoltorio ligero**, con algunas verificaciones básicas y *logging*, que puede servir de base para flujos de trabajo más complejos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0986e1af-8853-4c22-a0e2-09e3476c441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.py\n",
    "\n",
    "from mcp.types import Resource, ResourceTemplate, BlobResourceContents, TextResourceContents\n",
    "\n",
    "class MCPClient:\n",
    "    ...\n",
    "    async def get_available_resources(self) -> list[Resource]:\n",
    "        if not self._connected:\n",
    "            raise RuntimeError(\"Client not connected to a server\")\n",
    "\n",
    "        resources_result = await self._session.list_resources()\n",
    "        if not resources_result.resources:\n",
    "            logger.warning(\"No resources found on server\")\n",
    "        return resources_result.resources\n",
    "\n",
    "    async def get_available_resource_templates(self) -> list[ResourceTemplate]:\n",
    "        if not self._connected:\n",
    "            raise RuntimeError(\"Client not connected to a server\")\n",
    "\n",
    "        resource_templates_result = await self._session.list_resource_templates()\n",
    "        if not resource_templates_result.resources:\n",
    "            logger.warning(\"No resource templates found on server\")\n",
    "        return resource_templates_result.resources\n",
    "\n",
    "    async def get_resource(self, uri: str) -> list[BlobResourceContents | TextResourceContents]:\n",
    "        if not self._connected:\n",
    "            raise RuntimeError(\"Client not connected to a server\")\n",
    "        \n",
    "        resource_read_result = await self._session.read_resource(uri=uri)\n",
    "        if not resource_read_result.contents:\n",
    "            logger.warning(f\"No content read for resource URI {uri}\")\n",
    "        return resource_read_result.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11a0fe-2819-4989-abba-4811016c4588",
   "metadata": {},
   "source": [
    "\n",
    "Estos métodos son casi idénticos entre sí y al método `get_available_tools()`. Cada función:\n",
    "\n",
    "1. Verifica que exista una conexión antes de realizar solicitudes al servidor.\n",
    "2. Llama al método de listado apropiado (`list_resources()` o `list_resource_templates()`).\n",
    "3. Comprueba si hay resultados y emite una advertencia si la lista está vacía.\n",
    "4. Devuelve los resultados.\n",
    "\n",
    "Al igual que `list_tools()`, estos métodos aceptan un parámetro opcional `cursor` para **paginación de resultados**.\n",
    "\n",
    "---\n",
    "\n",
    "> **Nota**\n",
    "> Los recursos y las plantillas de recursos tienen casi el mismo conjunto de propiedades: `uri`, `name`, `description`, `mimeType`, `size`, `annotations` y `model_config`.\n",
    "> En las plantillas de recursos, en lugar de `uri`, se usa la propiedad `uriTemplate`.\n",
    "\n",
    "---\n",
    "\n",
    "El método `get_resource()` sigue el mismo patrón que `get_available_tools()`:\n",
    "no transforma los objetos MCP `BlobResourceContents` y `TextResourceContents`, sino que los devuelve tal cual, proporcionando información útil sobre el tipo de contenido y manteniendo la filosofía de **procesamiento mínimo**.\n",
    "\n",
    "---\n",
    "\n",
    "El protocolo MCP también permite que los clientes se **suscriban a notificaciones de cambios** en los recursos.\n",
    "Actualmente, en el SDK de Python, la gestión de notificaciones está **parcialmente implementada** y no existe forma de registrar manejadores específicos.\n",
    "En cambio, en el SDK de TypeScript sí hay soporte para callbacks en `ClientSession`, que manejan todas las entradas, incluidas solicitudes, respuestas y notificaciones.\n",
    "\n",
    "---\n",
    "\n",
    "Hasta ahora aprendiste:\n",
    "\n",
    "1. Cómo obtener una lista de **recursos disponibles** y **plantillas de recursos**.\n",
    "2. Cómo leer un recurso usando su URI.\n",
    "\n",
    "---\n",
    "\n",
    "### Uso en una aplicación anfitriona\n",
    "\n",
    "Los recursos se usan principalmente para **proporcionar contexto adicional** a las consultas de los usuarios, por ejemplo, agregando un script Bash a una pregunta sobre dicho script.\n",
    "\n",
    "En nuestro ejemplo de chatbot minimalista:\n",
    "\n",
    "* Buscamos la clave `\"context:\"` en el *prompt* del usuario.\n",
    "* Cuando se detecta, el contenido posterior se utiliza para seleccionar un recurso específico.\n",
    "* Ese recurso se agrega como **contexto** a la consulta original del usuario.\n",
    "\n",
    "Cuando la aplicación inicia o el usuario escribe `\"refresh\"`, la aplicación:\n",
    "\n",
    "1. Obtiene la lista actual de recursos del servidor.\n",
    "2. Los muestra al usuario.\n",
    "\n",
    "---\n",
    "\n",
    "El ejemplo de código está dividido en dos archivos: `client.py` y `agent.py`, para **reducir la complejidad** y mejorar la legibilidad.\n",
    "\n",
    "En `agent.py` ahora se encuentra la clase **Agent**, que incluye:\n",
    "\n",
    "* `run()` → anteriormente `main()`.\n",
    "* Funciones utilitarias para recursos:\n",
    "\n",
    "  * `extract_resource_name()` → extrae el nombre del recurso del *prompt* del usuario.\n",
    "  * `display_resources()` → muestra nombres y descripciones de los recursos disponibles.\n",
    "  * `refresh_resources()` → actualiza y devuelve los recursos disponibles como un diccionario.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39b69bf-2be4-44c3-a7df-791cb56835ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Anthropic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# agent.py\u001b[39;00m\n\u001b[32m      3\u001b[39m ...\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mAgent\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcp_client\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMCPClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manthropic_client\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAnthropic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmcp_client\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcp_client\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mAgent\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAgent\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mcp_client: MCPClient, anthropic_client: \u001b[43mAnthropic\u001b[49m):\n\u001b[32m      6\u001b[39m         \u001b[38;5;28mself\u001b[39m.mcp_client = mcp_client\n\u001b[32m      7\u001b[39m         \u001b[38;5;28mself\u001b[39m.anthropic_client = anthropic_client\n",
      "\u001b[31mNameError\u001b[39m: name 'Anthropic' is not defined"
     ]
    }
   ],
   "source": [
    "# agent.py\n",
    "\n",
    "...\n",
    "class Agent:\n",
    "    def __init__(self, mcp_client: MCPClient, anthropic_client: Anthropic):\n",
    "        self.mcp_client = mcp_client\n",
    "        self.anthropic_client = anthropic_client\n",
    "        self.available_resources = {}\n",
    "\n",
    "    async def _select_resources(self, prompt: str) -> list[str]:\n",
    "        \"\"\"Usar el LLM para seleccionar inteligentemente recursos relevantes.\"\"\"\n",
    "        if not self.available_resources:\n",
    "            return []\n",
    "\n",
    "        resource_descriptions = {\n",
    "            name: resource.description or f\"Resource: {name}\"\n",
    "            for name, resource in self.available_resources.items()\n",
    "        }\n",
    "\n",
    "        selection_prompt = f\"\"\"\n",
    "                        Dada esta pregunta del usuario: \"{prompt}\"\n",
    "                        \n",
    "                        Y estos recursos disponibles:\n",
    "                        {json.dumps(resource_descriptions, indent=2)}\n",
    "                        \n",
    "                        ¿Qué recursos (si los hay) serían útiles para responder la pregunta del usuario?\n",
    "                        Devuelve un arreglo JSON con los nombres de los recursos, o un arreglo vacío si no se necesitan recursos.\n",
    "                        Solo incluye recursos que sean directamente relevantes.\n",
    "                        \n",
    "                        Ejemplo: [\"math-constants\"] o []\n",
    "                        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.anthropic_client.messages.create(\n",
    "                max_tokens=200,\n",
    "                messages=[{\"role\": \"user\", \"content\": selection_prompt}],\n",
    "                model=\"claude-sonnet-4-0\",\n",
    "            )\n",
    "\n",
    "            response_text = response.content[0].text.strip()\n",
    "            # Extraer JSON de la respuesta (manejar caso donde el LLM añade explicación)\n",
    "            if \"[\" in response_text and \"]\" in response_text:\n",
    "                start = response_text.find(\"[\")\n",
    "                end = response_text.rfind(\"]\") + 1\n",
    "                json_part = response_text[start:end]\n",
    "                selected_resources = json.loads(json_part)\n",
    "                return [r for r in selected_resources if r in self.available_resources]\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"No se pudieron seleccionar recursos con el LLM: {e}\")\n",
    "\n",
    "        return []\n",
    "\n",
    "    async def _load_selected_resources(\n",
    "        self, resource_names: list[str]\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Cargar los recursos especificados.\"\"\"\n",
    "        context_messages = []\n",
    "\n",
    "        for resource_name in resource_names:\n",
    "            if resource_name in self.available_resources:\n",
    "                print(f\"LLM seleccionó el recurso: {resource_name}\")\n",
    "                try:\n",
    "                    resource = self.available_resources[resource_name]\n",
    "                    resource_contents = await self.mcp_client.get_resource(\n",
    "                        uri=resource.uri\n",
    "                    )\n",
    "                    for content in resource_contents:\n",
    "                        if isinstance(content, TextResourceContents):\n",
    "                            context_messages.append(\n",
    "                                {\n",
    "                                    \"type\": \"text\",\n",
    "                                    \"text\": f\"[Resource: {resource_name}]\\n{content.text}\",\n",
    "                                }\n",
    "                            )\n",
    "                        elif content.mimeType in [\n",
    "                            \"image/jpeg\",\n",
    "                            \"image/png\",\n",
    "                            \"image/gif\",\n",
    "                            \"image/webp\",\n",
    "                        ]:  # imagen codificada en base64\n",
    "                            context_messages.append(\n",
    "                                {\n",
    "                                    \"type\": \"image\",\n",
    "                                    \"source\": {\n",
    "                                        \"type\": \"base64\",\n",
    "                                        \"media_type\": content.mimeType,\n",
    "                                        \"data\": content.blob,\n",
    "                                    },\n",
    "                                }\n",
    "                            )\n",
    "                        else:\n",
    "                            print(\n",
    "                                f\"ADVERTENCIA: No se puede procesar mimeType {resource_contents.mimeType} para el recurso {resource_name}\"\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error cargando recurso {resource_name}: {e}\")\n",
    "\n",
    "        return context_messages\n",
    "\n",
    "    async def _refresh_resources(self) -> None:\n",
    "        available_resources = await self.mcp_client.get_available_resources()\n",
    "        self.available_resources = {\n",
    "            resource.name: resource for resource in available_resources\n",
    "        }\n",
    "\n",
    "    async def run(self):\n",
    "        try:\n",
    "            print(\n",
    "                \"Bienvenido a tu Asistente de IA. Escribe 'goodbye' para salir o 'refresh' para recargar y mostrar los recursos disponibles.\"\n",
    "            )\n",
    "            await self.mcp_client.connect()\n",
    "            available_tools = await self.mcp_client.get_available_tools()\n",
    "            await self._refresh_resources()\n",
    "\n",
    "            while True:\n",
    "                prompt = input(\"You: \")\n",
    "\n",
    "                if prompt.lower() == \"goodbye\":\n",
    "                    print(\"AI Assistant: ¡Adiós!\")\n",
    "                    break\n",
    "\n",
    "                if prompt.lower() == \"refresh\":\n",
    "                    await self._refresh_resources()\n",
    "                    continue\n",
    "\n",
    "                selected_resource_names = await self._select_resources(prompt)\n",
    "                context_messages = await self._load_selected_resources(\n",
    "                    selected_resource_names\n",
    "                )\n",
    "\n",
    "                # Construir conversación con mensaje inicial y contexto\n",
    "                user_content = [{\"type\": \"text\", \"text\": prompt}]\n",
    "                if context_messages:\n",
    "                    user_content.extend(context_messages)\n",
    "\n",
    "                conversation_messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "                # Bucle de uso de herramientas hasta obtener respuesta final de texto\n",
    "                while True:\n",
    "                    current_response = anthropic_client.messages.create(\n",
    "                        max_tokens=4096,\n",
    "                        messages=conversation_messages,\n",
    "                        model=\"claude-sonnet-4-0\",\n",
    "                        tools=available_tools,\n",
    "                        tool_choice={\"type\": \"auto\"},\n",
    "                    )\n",
    "\n",
    "                    conversation_messages.append(\n",
    "                        {\"role\": \"assistant\", \"content\": current_response.content}\n",
    "                    )\n",
    "\n",
    "                    if current_response.stop_reason == \"tool_use\":\n",
    "                        tool_use_blocks = [\n",
    "                            block\n",
    "                            for block in current_response.content\n",
    "                            if block.type == \"tool_use\"\n",
    "                        ]\n",
    "\n",
    "                        print(f\"Ejecutando {len(tool_use_blocks)} herramienta(s)...\")\n",
    "\n",
    "                        tool_results = []\n",
    "                        for tool_use in tool_use_blocks:\n",
    "                            print(f\"Usando herramienta: {tool_use.name}\")\n",
    "                            tool_result = await self.mcp_client.use_tool(\n",
    "                                tool_name=tool_use.name, arguments=tool_use.input\n",
    "                            )\n",
    "                            tool_results.append(\n",
    "                                {\n",
    "                                    \"type\": \"tool_result\",\n",
    "                                    \"tool_use_id\": tool_use.id,\n",
    "                                    \"content\": \"\\n\".join(tool_result),\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                        conversation_messages.append(\n",
    "                            {\"role\": \"user\", \"content\": tool_results}\n",
    "                        )\n",
    "                        continue\n",
    "                    else:\n",
    "                        text_blocks = [\n",
    "                            content.text\n",
    "                            for content in current_response.content\n",
    "                            if hasattr(content, \"text\") and content.text.strip()\n",
    "                        ]\n",
    "\n",
    "                        if text_blocks:\n",
    "                            print(f\"Assistant: {text_blocks[0]}\")\n",
    "                        else:\n",
    "                            print(\"Assistant: [No hay respuesta de texto disponible]\")\n",
    "\n",
    "                        break\n",
    "        finally:\n",
    "            await self.mcp_client.disconnect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc0ae89-8673-466c-83f5-29c0fa3e9cad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     mcp_client = MCPClient(\n\u001b[32m      3\u001b[39m         name=\u001b[33m\"\u001b[39m\u001b[33mcalculator_server_connection\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m         command=\u001b[33m\"\u001b[39m\u001b[33muv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m         server_args=[\n\u001b[32m      6\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m--directory\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m             \u001b[38;5;28mstr\u001b[39m(\u001b[43mPath\u001b[49m(\u001b[34m__file__\u001b[39m).parent.parent.resolve()),\n\u001b[32m      8\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrun\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcalculator_server.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m         ],\n\u001b[32m     11\u001b[39m     )\n\u001b[32m     12\u001b[39m     agent = Agent(mcp_client, anthropic_client)\n\u001b[32m     13\u001b[39m     asyncio.run(agent.run())\n",
      "\u001b[31mNameError\u001b[39m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    mcp_client = MCPClient(\n",
    "        name=\"calculator_server_connection\",\n",
    "        command=\"uv\",\n",
    "        server_args=[\n",
    "            \"--directory\",\n",
    "            str(Path(__file__).parent.parent.resolve()),\n",
    "            \"run\",\n",
    "            \"calculator_server.py\",\n",
    "        ],\n",
    "    )\n",
    "    agent = Agent(mcp_client, anthropic_client)\n",
    "    asyncio.run(agent.run())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cec1c76-62e5-4ecb-b34c-18404e8fc006",
   "metadata": {},
   "source": [
    "Esta integración específica comenzó igual que la integración que hiciste en la sección de Herramientas: conectarse al servidor MCP a través del cliente, descubrir los primitivos disponibles, como recursos en este caso, y cargar esos primitivos en memoria. Añadimos dos nuevas funciones utilitarias al agente:\n",
    "\n",
    "- `_select_resources()`: Crea un diccionario nombre: descripción de los recursos disponibles y luego usa el LLM para seleccionar cualquier recurso relevante dado el prompt del usuario.\n",
    "\n",
    "- `_load_selected_resources()`: Toma una lista de nombres de recursos de `_select_resources()` y, para los nombres válidos, usa el cliente para obtener los contenidos del recurso. Si esos contenidos son texto, creamos un bloque de contenido con el texto. Si es un blob, verificamos que el `mimeType` coincida con alguno de los tipos de imagen soportados por la API del modelo de Anthropic y, si es así, construimos un bloque de contenido con esos datos y lo añadimos a la lista final de mensajes que se enviarán al llamador.\n",
    "\n",
    "- `_refresh_resources()`: Actualiza los recursos disponibles y los almacena en el diccionario `available_resources` del objeto Agent.\n",
    "\n",
    "En la función `run()`, actualizamos el mensaje de bienvenida para informar al usuario que un mensaje con solo la palabra `refresh` actualizará la lista de recursos. La lista de recursos se llena con los resultados del método `_refresh_resources()`. Luego, cuando el usuario envía un prompt, el LLM selecciona los recursos pertinentes mediante `_select_resources()`. Después, llamamos a `_load_selected_resources()` para añadir cualquier recurso seleccionado a la lista de `context_messages`. Los mensajes de contexto proporcionan contexto adicional al LLM, y los combinamos con el prompt del usuario en una lista. Esta lista de prompt del usuario y recursos seleccionados se envía al LLM, que podrá usar el contenido de los recursos para responder al prompt del usuario si es necesario.\n",
    "\n",
    "> NOTA  \n",
    "Al añadir contexto a un prompt del usuario, no deberías crear un mensaje de usuario separado como con el prompt original. En su lugar, convierte el mensaje del usuario a su forma completa: un diccionario anidado con la clave `type` y la clave `content`, que contiene una lista de diccionarios, y añade cualquier mensaje de contexto a esa lista.\n",
    "\n",
    "Hay varias mejoras que puedes hacer en este ejemplo. Para ampliar tus conocimientos, intenta implementar soporte para listar y usar plantillas de recursos junto con recursos normales. Luego, intenta soportar múltiples archivos de contexto en lugar de solo uno, ya sea con parsing de texto regular o incluso con otra llamada a un LLM. También puedes optimizar: actualmente, nuestra aplicación hace una llamada adicional potencialmente costosa al LLM por cada entrada del usuario para seleccionar los recursos más relevantes para su consulta. ¿Cómo podrías hacer esto más eficiente?\n",
    "\n",
    "> NOTA  \n",
    "Como habrás notado, muchas de estas mejoras son prácticas típicas de ingeniería de software y no son específicas de IA generativa. Aunque la ingeniería de IA se está convirtiendo en una disciplina propia, sigue siendo ingeniería de software en esencia, y las prácticas y herramientas que tienes como ingeniero de software te servirán también para construir aplicaciones de IA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f643ef-0b3e-4b35-a430-48e81925d10c",
   "metadata": {},
   "source": [
    "\n",
    "# Prompts\n",
    "\n",
    "Los **prompts** son el último de los tres principales primitivos que proveen los servidores MCP. Están diseñados para ser controlados por el usuario de la aplicación, quien debería poder seleccionar qué prompt o prompts desea usar, similar a la interfaz que viste en el ejemplo de la aplicación host en la sección anterior.  \n",
    "\n",
    "Cada definición de prompt tiene:  \n",
    "- Un identificador único `name`.  \n",
    "- Una descripción opcional legible por humanos.  \n",
    "- Argumentos opcionales: una lista de diccionarios con:\n",
    "  - `name`\n",
    "  - descripción legible opcional\n",
    "  - booleano `required` opcional indicando si el argumento es obligatorio.  \n",
    "\n",
    "Corresponde a la implementación del cliente o la aplicación host que use el prompt inyectar valores para cada uno de los argumentos. Estos se llaman **prompts dinámicos**.\n",
    "\n",
    "---\n",
    "\n",
    "## Manejo de prompts con el SDK de Python\n",
    "\n",
    "El SDK de Python simplifica el manejo de prompts mediante:  \n",
    "\n",
    "- `Session.list_prompts()`: devuelve una lista de objetos `Prompt` con las propiedades mencionadas.  \n",
    "- `get_prompt()`: permite obtener un prompt específico proporcionando su `name` y argumentos, devolviendo una lista de objetos `PromptMessage` listos para usar.  \n",
    "\n",
    "Los **PromptMessage** pueden convertirse en diccionarios simples y enviarse directamente al LLM. Cada mensaje enviado al LLM tiene:  \n",
    "- Claves `role` y `content` (requeridas).  \n",
    "- `content` puede contener:\n",
    "  - `TextContent`\n",
    "  - `ImageContent`\n",
    "  - `AudioContent`\n",
    "  - `EmbeddedResource`  \n",
    "\n",
    "Los **EmbeddedResource** son recursos incrustados dentro del bloque de contenido del mensaje, tal como en el ejemplo de la aplicación host de la sección anterior.\n",
    "\n",
    "---\n",
    "\n",
    "## Ejemplo de cliente MCP para prompts\n",
    "\n",
    "```python\n",
    "# client.py\n",
    "from mcp.types import Prompt\n",
    "\n",
    "class MCPClient:\n",
    "    ...\n",
    "    async def get_available_prompts(self) -> list[Prompt]:\n",
    "        if not self._connected:\n",
    "            raise RuntimeError(\"Client not connected to a server\")\n",
    "\n",
    "        prompt_result = await self._session.list_prompts()\n",
    "        if not prompt_result.prompts:\n",
    "            logger.warning(\"No prompts found on server\")\n",
    "        return prompt_result.prompts\n",
    "````\n",
    "\n",
    "Este patrón es similar al de recursos y herramientas:\n",
    "\n",
    "1. Se verifica la conexión.\n",
    "2. Se obtiene la lista de prompts del servidor.\n",
    "3. Se asegura que la lista no esté vacía antes de devolverla al usuario.\n",
    "\n",
    "El wrapper `get_prompt()` funciona de forma similar: se obtiene el nombre y los argumentos del usuario, se carga el prompt desde el servidor y se devuelve el objeto `PromptMessage` al cliente.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "558a9b8e-7133-46ed-b673-223d99a1b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.py\n",
    "\n",
    "from mcp.types import PromptMessage\n",
    "\n",
    "class MCPClient:\n",
    "    ...\n",
    "    async def load_prompt(\n",
    "        self, name: str, arguments: dict[str, str]\n",
    "    ) -> list[PromptMessage]:\n",
    "        if not self._connected:\n",
    "            raise RuntimeError(\"Client not connected to a server\")\n",
    "        prompt_load_result = await self._session.get_prompt(name=name, arguments=arguments)\n",
    "\n",
    "        if not prompt_load_result.messages:\n",
    "            logger.warning(f\"No prompt found for prompt {name}\")\n",
    "        else:\n",
    "            logger.warning(f\"Loaded prompt {name} with description {prompt_load_result.description}\")\n",
    "        return prompt_load_result.messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3c006-cf7d-4b49-b9c1-0254a10c82b2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "* Esta función es similar a cómo se recuperan los recursos.\n",
    "* Incluye controles estándar de conexión y pasa el `name` y `arguments` directamente al método `get_prompt()` de la sesión.\n",
    "* Se agrega logging para visibilidad.\n",
    "\n",
    "**Nota sobre notificaciones:**\n",
    "\n",
    "* El protocolo MCP permite notificaciones cuando cambia la lista de prompts disponibles.\n",
    "* Actualmente, no hay API de usuario para manejar estas notificaciones, aunque los clientes pueden suscribirse a ellas.\n",
    "* Para usarlas, habría que enrutar las notificaciones a un callback propio, siguiendo ejemplos en el SDK (como `Session._received_request()`) o creando una función que maneje todos los mensajes y pasándola al constructor del cliente en `message_handler`.\n",
    "\n",
    "---\n",
    "\n",
    "# Integración en la aplicación host\n",
    "\n",
    "* Se reutiliza la misma base de aplicación que en los ejemplos anteriores.\n",
    "* Se obtiene la lista de prompts disponibles.\n",
    "* Se muestran al usuario para que seleccione cuál usar.\n",
    "* El prompt seleccionado se envía tal cual al LLM y se maneja la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dae6c47-e3e7-4c27-b02e-4b247c2d1b00",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MCPClient' from 'mcp.client' (C:\\Users\\mcssa\\.conda\\envs\\google-adk\\Lib\\site-packages\\mcp\\client\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmcp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MCPClient\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01manthropic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Anthropic\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Clase principal del agente\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'MCPClient' from 'mcp.client' (C:\\Users\\mcssa\\.conda\\envs\\google-adk\\Lib\\site-packages\\mcp\\client\\__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# agent.py\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "from mcp.client import MCPClient\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# ============================================================\n",
    "# Clase principal del agente\n",
    "# ============================================================\n",
    "class Agent:\n",
    "    def __init__(self, mcp_client: MCPClient, anthropic_client: Anthropic):\n",
    "        self.mcp_client = mcp_client\n",
    "        self.anthropic_client = anthropic_client\n",
    "        self.available_resources = {}\n",
    "        self.available_prompts = {}\n",
    "\n",
    "    # ========================================================\n",
    "    # Selección de recursos con LLM\n",
    "    # ========================================================\n",
    "    async def _select_resources(self, user_query: str) -> list[str]:\n",
    "        \"\"\"Usa el LLM para seleccionar recursos relevantes.\"\"\"\n",
    "        ...\n",
    "    \n",
    "    # ========================================================\n",
    "    # Selección de prompts con LLM\n",
    "    # ========================================================\n",
    "    async def _select_prompts(self, user_query: str) -> list[dict[str, Any]]:\n",
    "        \"\"\"Usa el LLM para seleccionar prompts relevantes.\"\"\"\n",
    "        if not self.available_prompts:\n",
    "            return []\n",
    "\n",
    "        prompts = [prompt.model_dump_json() for prompt in self.available_prompts.values()]\n",
    "\n",
    "        selection_prompt = f\"\"\"\n",
    "        Given this user question: \"{user_query}\"\n",
    "        \n",
    "        And these available prompt templates:\n",
    "        {json.dumps(prompts, indent=2)}\n",
    "        \n",
    "        Which prompts (if any) would provide helpful instructions or guidance for answering this question?\n",
    "        Return a JSON array of prompt objects which have a name (string) and arguments (objects where the\n",
    "        keys are the named parameter name and value is the argument value), or an empty array if no prompts\n",
    "        are needed. Only include prompts that are directly relevant.\n",
    "        \n",
    "        Example: [{{\"name\": \"calculation-helper\", \"arguments\": {{\"operation\": \"addition\"}}}},\n",
    "                 {{\"name\": \"step-by-step-math\", \"arguments\": {{}}}}] or []\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.anthropic_client.messages.create(\n",
    "                max_tokens=200,\n",
    "                messages=[{\"role\": \"user\", \"content\": selection_prompt}],\n",
    "                model=\"claude-sonnet-4-0\",\n",
    "            )\n",
    "\n",
    "            response_text = response.content[0].text.strip()\n",
    "            if \"[\" in response_text and \"]\" in response_text:\n",
    "                start = response_text.find(\"[\")\n",
    "                end = response_text.rfind(\"]\") + 1\n",
    "                json_part = response_text[start:end]\n",
    "                selected_prompts = json.loads(json_part)\n",
    "                return [p for p in selected_prompts if p[\"name\"] in self.available_prompts]\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to select prompts with LLM: {e}\")\n",
    "\n",
    "        return []\n",
    "\n",
    "    # ========================================================\n",
    "    # Carga de recursos seleccionados\n",
    "    # ========================================================\n",
    "    async def _load_selected_resources(self, resource_names: list[str]) -> list[dict[str, Any]]:\n",
    "        \"\"\"Carga los recursos seleccionados (pendiente de implementación).\"\"\"\n",
    "        ...\n",
    "\n",
    "    # ========================================================\n",
    "    # Carga de prompts seleccionados\n",
    "    # ========================================================\n",
    "    async def _load_selected_prompts(self, prompts: list[dict[str, Any]]) -> str:\n",
    "        \"\"\"Carga los prompts seleccionados como instrucciones del sistema.\"\"\"\n",
    "        system_instructions = []\n",
    "\n",
    "        for prompt in prompts:\n",
    "            if prompt[\"name\"] in self.available_prompts:\n",
    "                print(f\"Using prompt: {prompt['name']}\")\n",
    "                try:\n",
    "                    prompt_content = await self.mcp_client.load_prompt(\n",
    "                        name=prompt[\"name\"], arguments=prompt[\"arguments\"]\n",
    "                    )\n",
    "\n",
    "                    # Extraer el texto del prompt\n",
    "                    prompt_text = \"\"\n",
    "                    for message in prompt_content:\n",
    "                        if hasattr(message.content, \"text\"):\n",
    "                            prompt_text += message.content.text + \"\\n\"\n",
    "                        elif isinstance(message.content, str):\n",
    "                            prompt_text += message.content + \"\\n\"\n",
    "\n",
    "                    if prompt_text.strip():\n",
    "                        system_instructions.append(\n",
    "                            f\"[Prompt: {prompt['name']}]\\n{prompt_text.strip()}\"\n",
    "                        )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading prompt {prompt['name']}: {e}\")\n",
    "\n",
    "        return \"\\n\\n\".join(system_instructions)\n",
    "\n",
    "    # ========================================================\n",
    "    # Actualización de recursos y prompts disponibles\n",
    "    # ========================================================\n",
    "    async def _refresh(self) -> None:\n",
    "        \"\"\"Obtiene y actualiza los recursos y prompts disponibles del cliente MCP.\"\"\"\n",
    "        available_resources = await self.mcp_client.get_available_resources()\n",
    "        self.available_resources = {resource.name: resource for resource in available_resources}\n",
    "\n",
    "        available_prompts = await self.mcp_client.get_available_prompts()\n",
    "        self.available_prompts = {prompt.name: prompt for prompt in available_prompts}\n",
    "\n",
    "    # ========================================================\n",
    "    # Bucle principal de ejecución del agente\n",
    "    # ========================================================\n",
    "    async def run(self):\n",
    "        \"\"\"Ejecuta el agente en modo interactivo.\"\"\"\n",
    "        try:\n",
    "            print(\n",
    "                \"Welcome to your AI Assistant. Type 'goodbye' to quit or 'refresh' to reload and redisplay available resources.\"\n",
    "            )\n",
    "            await self.mcp_client.connect()\n",
    "            available_tools = await self.mcp_client.get_available_tools()\n",
    "            await self._refresh()\n",
    "\n",
    "            print(\n",
    "                f\"Loaded {len(self.available_resources)} resources and {len(self.available_prompts)} prompts\"\n",
    "            )\n",
    "\n",
    "            while True:\n",
    "                prompt = input(\"You: \")\n",
    "\n",
    "                if prompt.lower() == \"goodbye\":\n",
    "                    print(\"AI Assistant: Goodbye!\")\n",
    "                    break\n",
    "\n",
    "                if prompt.lower() == \"refresh\":\n",
    "                    await self._refresh()\n",
    "                    continue\n",
    "\n",
    "                # ===============================================\n",
    "                # Selección y carga de recursos y prompts\n",
    "                # ===============================================\n",
    "                selected_resource_names = await self._select_resources(prompt)\n",
    "                selected_prompt_names = await self._select_prompts(prompt)\n",
    "\n",
    "                context_messages = await self._load_selected_resources(selected_resource_names)\n",
    "                system_instructions = await self._load_selected_prompts(selected_prompt_names)\n",
    "\n",
    "                # ===============================================\n",
    "                # Construcción de la conversación\n",
    "                # ===============================================\n",
    "                user_content = [{\"type\": \"text\", \"text\": prompt}]\n",
    "                if context_messages:\n",
    "                    user_content.extend(context_messages)\n",
    "\n",
    "                conversation_messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "                # ===============================================\n",
    "                # Bucle de interacción con el modelo y herramientas\n",
    "                # ===============================================\n",
    "                while True:\n",
    "                    create_message_args = {\n",
    "                        \"max_tokens\": 4096,\n",
    "                        \"messages\": conversation_messages,\n",
    "                        \"model\": \"claude-sonnet-4-0\",\n",
    "                        \"tools\": available_tools,\n",
    "                        \"tool_choice\": {\"type\": \"auto\"},\n",
    "                    }\n",
    "\n",
    "                    if system_instructions:\n",
    "                        create_message_args[\"system\"] = system_instructions\n",
    "\n",
    "                    current_response = self.anthropic_client.messages.create(**create_message_args)\n",
    "\n",
    "                    # Agregar respuesta del asistente a la conversación\n",
    "                    conversation_messages.append({\"role\": \"assistant\", \"content\": current_response.content})\n",
    "\n",
    "                    # ===============================================\n",
    "                    # Manejo de uso de herramientas\n",
    "                    # ===============================================\n",
    "                    if current_response.stop_reason == \"tool_use\":\n",
    "                        tool_use_blocks = [block for block in current_response.content if block.type == \"tool_use\"]\n",
    "\n",
    "                        tool_results = []\n",
    "                        for tool_use in tool_use_blocks:\n",
    "                            print(f\"Using tool: {tool_use.name}\")\n",
    "                            tool_result = await self.mcp_client.use_tool(\n",
    "                                tool_name=tool_use.name,\n",
    "                                arguments=tool_use.input\n",
    "                            )\n",
    "                            tool_results.append(\n",
    "                                {\n",
    "                                    \"type\": \"tool_result\",\n",
    "                                    \"tool_use_id\": tool_use.id,\n",
    "                                    \"content\": \"\\n\".join(tool_result),\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                        conversation_messages.append({\"role\": \"user\", \"content\": tool_results})\n",
    "                        continue\n",
    "\n",
    "                    # ===============================================\n",
    "                    # Respuesta final del asistente\n",
    "                    # ===============================================\n",
    "                    else:\n",
    "                        text_blocks = [\n",
    "                            content.text for content in current_response.content\n",
    "                            if hasattr(content, \"text\") and content.text.strip()\n",
    "                        ]\n",
    "\n",
    "                        if text_blocks:\n",
    "                            print(f\"Assistant: {text_blocks[0]}\")\n",
    "                        else:\n",
    "                            print(\"Assistant: [No text response available]\")\n",
    "\n",
    "                        break\n",
    "        finally:\n",
    "            await self.mcp_client.disconnect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac37524-8455-49e5-adfc-609aaf9c66e5",
   "metadata": {},
   "source": [
    "# Integración de Prompts en el Agente\n",
    "\n",
    "Esta versión del código realiza todas las acciones descritas en la introducción del ejemplo.\n",
    "\n",
    "### 1. Bucle principal e inicialización\n",
    "En los bucles principales del agente, primero se llama a la función `_refresh()`, la cual invoca el método `get_available_prompts()` que implementamos en el cliente.  \n",
    "Esta función llena el atributo `available_prompts` con un diccionario similar al del ejemplo de **Resources**, donde:\n",
    "- Las **claves** son los nombres de los prompts.  \n",
    "- Los **valores** son los objetos `Prompt`.\n",
    "\n",
    "### 2. Selección de prompts relevantes\n",
    "Dentro del bucle del agente se llama a `_select_prompts()` para que el modelo LLM seleccione los prompts más relevantes para la consulta del usuario, de forma similar al ejemplo de **Resources**, pero con algunas diferencias:\n",
    "- Cada objeto `Prompt` se convierte en una cadena JSON usando `model_dump_json()`.  \n",
    "- Esto permite que el prompt de selección incluya toda la información del objeto (descripción y argumentos), ayudando al modelo a decidir cuáles prompts son los más adecuados.\n",
    "\n",
    "### 3. Carga y validación de prompts seleccionados\n",
    "Luego se llama a `_load_selected_prompts()` para:\n",
    "- Filtrar los prompts alucinados (no válidos).  \n",
    "- Obtener los prompts restantes mediante el cliente.  \n",
    "- Transformarlos en el formato de **prompts del sistema** que utiliza el LLM.\n",
    "\n",
    "### 4. Preparación de la llamada al LLM\n",
    "Antes de llamar al modelo LLM:\n",
    "1. Se crea un diccionario con los argumentos del mensaje.  \n",
    "2. Si existen prompts cargados, se añaden bajo la clave `system`.  \n",
    "3. Finalmente, estos argumentos se desempaquetan en la llamada `Anthropic.create()`.\n",
    "\n",
    "### 5. Resto del agente\n",
    "El resto del agente conserva la misma lógica del ejemplo anterior, incluyendo:\n",
    "- La interacción con el usuario.  \n",
    "- El uso de herramientas.  \n",
    "- La construcción del historial de conversación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7888e4-3c85-4e97-979f-a1d2156dbe4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (google-adk)",
   "language": "python",
   "name": "google-adk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
